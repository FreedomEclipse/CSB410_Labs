{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4f34b74",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/username/repo/blob/main/path-to-file)  \n",
    "**Students:** Replace `username`, `repo`, and `path-to-file` with your own GitHub username, repository name, and the path to this file.  \n",
    "After opening in Colab, go to **File → Save a copy to GitHub** (your repo) before editing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbe37d6",
   "metadata": {},
   "source": [
    "# Lab 6 (Week 7) — Convolutional Neural Networks (CNNs) — Student v4\n",
    "\n",
    "This lab maps directly to your Week 7 notebooks:\n",
    "- `7-lenet_in_keras.ipynb`\n",
    "- `7-alexnet_in_keras.ipynb`\n",
    "- `7-transfer_learning_in_keras.ipynb`\n",
    "\n",
    "**Sections**\n",
    "- F.1 LeNet-5 style CNN in Keras (MNIST)\n",
    "- F.2 AlexNet-like CNN in Keras (small image dataset, e.g., CIFAR-10)\n",
    "- F.3 Transfer Learning & Fine-Tuning with a Pretrained CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8125015b",
   "metadata": {},
   "source": [
    "## F.1 LeNet-5 Style CNN (Keras, MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fd3d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions only — write your own code. Reference: 7-lenet_in_keras.ipynb\n",
    "# Goal: Build and train a small LeNet-style CNN on MNIST.\n",
    "#\n",
    "# What to do:\n",
    "# 1) Load MNIST via keras.datasets; normalize to [0,1]; reshape to (N,28,28,1).\n",
    "# 2) Build a LeNet-style model, e.g.:\n",
    "#       Conv2D(32,(5,5),activation='relu') -> MaxPool2D(2,2)\n",
    "#       Conv2D(64,(5,5),activation='relu') -> MaxPool2D(2,2)\n",
    "#       Flatten -> Dense(120,'relu') -> Dense(84,'relu') -> Dense(10,'softmax')\n",
    "# 3) Compile: optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'].\n",
    "# 4) Train 2–3 epochs (validation_split=0.1, batch_size=128).\n",
    "# 5) Evaluate on test set; print test accuracy. Also print model.summary().\n",
    "#\n",
    "# Hints:\n",
    "# - Follow the exact preprocessing and layer pattern in 7-lenet_in_keras.ipynb.\n",
    "# - If your script used tanh/sigmoid, keep consistent unless instructed otherwise.\n",
    "#\n",
    "# Expected output:\n",
    "# - A model summary (Conv/Pool layers followed by Dense layers).\n",
    "# - Test accuracy typically ≥ 0.98 after a few epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacbf378",
   "metadata": {},
   "source": [
    "## F.2 AlexNet-like CNN (Keras, Small Image Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da210f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions only — write your own code. Reference: 7-alexnet_in_keras.ipynb\n",
    "# Goal: Adapt an AlexNet-like architecture to a smaller dataset (e.g., CIFAR-10).\n",
    "#\n",
    "# What to do:\n",
    "# 1) Choose dataset (e.g., CIFAR-10). Normalize images to [0,1]; ensure shape (N,H,W,3).\n",
    "# 2) Build an AlexNet-like model. Example skeleton (downscaled for small images):\n",
    "#       Conv2D(64,(3,3),strides=1,activation='relu',padding='same')\n",
    "#       MaxPool2D(2,2)\n",
    "#       Conv2D(192,(3,3),activation='relu',padding='same')\n",
    "#       MaxPool2D(2,2)\n",
    "#       Conv2D(384,(3,3),activation='relu',padding='same')\n",
    "#       Conv2D(256,(3,3),activation='relu',padding='same')\n",
    "#       Conv2D(256,(3,3),activation='relu',padding='same')\n",
    "#       MaxPool2D(2,2)\n",
    "#       Flatten -> Dense(1024,'relu') -> Dropout(0.5)\n",
    "#                -> Dense(1024,'relu') -> Dropout(0.5)\n",
    "#                -> Dense(num_classes,'softmax')\n",
    "# 3) Compile with Adam(1e-3), loss='sparse_categorical_crossentropy'.\n",
    "# 4) Train ~3 epochs (validation_split=0.1, batch_size=128); print test accuracy and model.summary().\n",
    "#\n",
    "# Hints:\n",
    "# - See layer ordering and hyperparameters in 7-alexnet_in_keras.ipynb.\n",
    "# - Adjust kernel sizes/strides for small images as shown in the script.\n",
    "#\n",
    "# Expected output:\n",
    "# - Model summary showing stacked conv blocks and large dense layers.\n",
    "# - Test accuracy after a few epochs (values vary; short runs ~0.65–0.80 on CIFAR-10).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc626ba",
   "metadata": {},
   "source": [
    "## F.3 Transfer Learning & Fine-Tuning (Pretrained CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49c074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions only — write your own code. Reference: 7-transfer_learning_in_keras.ipynb\n",
    "# Goal: Use a pretrained ImageNet model as a feature extractor, then fine-tune.\n",
    "#\n",
    "# What to do:\n",
    "# 1) Pick a pretrained base, e.g.:\n",
    "#       base = keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "# 2) Add a classification head:\n",
    "#       GlobalAveragePooling2D -> Dense(num_classes,'softmax')\n",
    "# 3) Freeze base; train head for 2–3 epochs.\n",
    "# 4) Unfreeze top blocks; fine-tune with low lr (e.g., 1e-5) for 1–2 epochs.\n",
    "# 5) Print train/val accuracy each epoch and final test accuracy.\n",
    "#\n",
    "# Hints:\n",
    "# - Follow the exact base model, preprocessing, and dataset loader used in 7-transfer_learning_in_keras.ipynb.\n",
    "#\n",
    "# Expected output:\n",
    "# - Logs showing initial training with frozen base and improved accuracy after fine-tuning.\n",
    "# - Final test accuracy higher than training from scratch on the same small dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13357f7b",
   "metadata": {},
   "source": [
    "## Discussion (answer in complete sentences)\n",
    "\n",
    "1. Compare LeNet and AlexNet design choices (kernel sizes, depth, FC layers). How do these affect compute and accuracy?  \n",
    "2. Why does transfer learning typically outperform training from scratch on small datasets? Which layers would you fine-tune and why?  \n",
    "3. If your AlexNet-like model underperforms, which two architecture or training changes would you try first, and why?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab 6 (Week 7) — Student v4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
